{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"tutorial2_1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2.1: Tokenization, Lemmatization, Stemming, and StopWords\n",
    "\n",
    "Welcome to Tutorial 2.1!  In today's class we covered fundamentals of text processing and used `nltk` and `spacy`. \n",
    "\n",
    "In this tutorial, we will go deeper into `spacy`, `nltk`, and other libraries used to process text. \n",
    "\n",
    "First, set up the tests and imports by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, but please don't change it.\n",
    "\n",
    "# These lines load the tests.\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textblob\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Karen Sparck Jones\n",
    "\n",
    "The text we will use today is the NYTimes obituary for Karen Sparck Jones.\n",
    "Make sure to first read the [obituary](https://www.nytimes.com/2019/01/02/obituaries/karen-sparck-jones-overlooked.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_1\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question 1.1:** In at most 3 sentences, who was Karen Sparck Jones and what is her connection to this course?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_2\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "We have extracted Karen Sparck Jones's obituary from the NYTimes and stored it in `data/sparck-jones-obit.txt`.\n",
    "\n",
    "**Question 1.2:** Read in the obituary and store the obituary as a single string in the variable named `obit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit = ...\n",
    "\" \".join(obit.split()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_3\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 1.3:** Use nltk's sentence tokenizer to convert the obituary into a list of sentences and store the first 3 sentences in the variable called `snippet`. `snippet` should be a single string where the sentences are concatenated and seperated by a white space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet = ...\n",
    "snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use three different nlp libraries to tokenize this snippet:\n",
    "\n",
    "- nltk\n",
    "- spacy\n",
    "- Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_1\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "-->\n",
    "\n",
    "**Question 2.1:** Use the default word tokenizer in nltk to tokenize the snippet. Store the list of tokens in a list called `nltk_tokens`. Make sure to first lowercase the text before applying the tokenizer.\n",
    "\n",
    "*Hint: look at the completed Demo 05 to see an example of how we tokenized text* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = ...\n",
    "nltk_tokens[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spacy\n",
    "\n",
    "Spacy is another powerful and popular python library for working with text. \n",
    "This [Cheat Sheet](http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06) is a good reference for looking up how to do different things with text using spacy. We've included a link to this cheat sheet on the course webpage.\n",
    "<br>\n",
    "We will now walk through Spacy briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Spacy Model\n",
    "\n",
    "Spacy has released pre-built models to tokenize, lemmatize, parse, and do other things with text, including extract Named Entities. On the top of this notebook, we downloaded the `'en_core_web_sm'` models, which is one of these models to work with English text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "The [Spacy documentation](https://spacy.io/models/en) describes this and other models. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_2\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question: 2.2:** Based on the documentation, what type of texts what the model trained on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "Run the next line to load the spacy english model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation, this `nlp` object is a text-processing pipeline. \n",
    "As the documentation mentions, \"*usually you'll load this once per process,\n",
    "and pass the instance around your application*.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy Doc & Token objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line will run the spacy NLP pipeline on this sentence. This will create a [Doc ojbect](https://spacy.io/api/doc) which we assign to the variable named `example_doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc = nlp('The next line will run the spacy NLP pipeline on this sentence')\n",
    "example_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "A spacy Doc object is a container for accessing linguistic annotations.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_3\n",
    "points: 1\n",
    "manual: false\n",
    "-->\n",
    "\n",
    "**Question 2.3:** In the next cell, determine how many tokens are in `example_doc` and assign the value to the variable named `number_example_tokens`.\n",
    "\n",
    "*Hint:* How do we determine the number of elements in a list or dictionary?\n",
    "\n",
    "*Note:* The test here does not check the number, we have kept that on gradescope as a hidden test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_example_tokens = ...\n",
    "f\"There are {number_example_tokens} tokens in the sentence \\\"{example_doc}\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When we iterate through a Doc object, we access each [Token object](https://spacy.io/api/token)  one at a time. A spacy Doc object is sequence, i.e. an ordered collection, of Token objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access these Token objects with indexing as seen in the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_first = example_doc[0]\n",
    "token_last = example_doc[-1]\n",
    "\n",
    "token_first, token_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the previous line printed out the words, these Token objects contain a lot of information beyond the surface form word. Run the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_first.lemma, token_first.lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_4\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question 2.4:** Briefly explain what was printed out on the line above. Feel free to look at the Token object documentation [online](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "The next cell shows some more information we can get from a Token object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_first.lower_)\n",
    "print(token_first.is_stop)\n",
    "print(token_first.is_currency)\n",
    "print(token_first.is_alpha)\n",
    "print(token_first.i)\n",
    "print(token_first.like_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the [documentation](https://spacy.io/api/token) on the Spacy website to familarize with all the more types of information that is in a token object. We will use this later in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing Doc objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we can get a subset of an array with slicing as shown in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(10)[3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_5\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "\n",
    "**Question 2.5:** When we slice a numpy array or a python list, what type is returned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "We can similarly slice a Document object.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_6\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "\n",
    "**Question 2.6:** In the next cell, use index slicing to get the 3rd, 4th, and 5th Tokens from `example_doc` and\n",
    "assign it to the variable named `example_sliced_doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sliced_doc = ...\n",
    "example_sliced_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`example_slided_doc` is not a new Document object, rather it is a Spacy [Span](https://spacy.io/api/span#attributes) object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(example_sliced_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The next line applies the `nlp` pipeline on the snippet from earlier in the assignment.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_7\n",
    "points: \n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 0.5\n",
    "    - 0.5\n",
    "-->\n",
    "\n",
    "\n",
    "**Question 2.7:** Complete the cell to loop through the spacy [Doc ojbect](https://spacy.io/api/doc) called `doc` and add the lower cased text of each token to the list called `spacy_tokens`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = []\n",
    "doc = nlp(snippet)\n",
    "\n",
    "...\n",
    "spacy_tokens[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textblob is another python library commonly used for processing text. Running the next line will run the snippet through the NLP pipeline using textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = textblob.TextBlob(snippet.lower())\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line will print out a textblob Object's functions and attirbutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(dir(blob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_tokens = list(blob.tokens)\n",
    "textblob_tokens[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_8\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "\n",
    "**Question 2.8:** In the next cell, write an expression to determine if the 3 lists of tokens each have the same number of tokens. Assign the boolean value to `same_tok_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tok_number = ...\n",
    "same_tok_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "It is possible that the different libraries result in different lists of tokens. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_9\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "\n",
    "**Question 2.9:** If the lists are different, in the next cell write what tokens are different and why might this be the case? If the lists are exactly the same, indicate that in the next cell.\n",
    "\n",
    "*You are encouraged to create a new python cell to write code to determine what words are different.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## 3. Part of Speech Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We are going to look at just the first two sentence of the obituary and explore different types of part of speech tags.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_1\n",
    "points: 1\n",
    "manual: false\n",
    "-->\n",
    "\n",
    "**Question 3.1:** Use nltk's sentence tokenizer to convert the obituary into a list of sentences and store the first 2 sentences in the variable called `short_snippet`. `short_snippet` should be a single string where the sentences are concatenated and seperated by a white space. Also, make sure to lowercase the text.\n",
    "\n",
    "Make sure not to word tokenize the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_snippet = ...\n",
    "short_snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will create a dataframe with part of speech tags for each token. \n",
    "There are different set of labels used for part of speech tagging. Here, we will look at the \n",
    "[Penn Treebank (PTB)](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "and a simplified version of the [Universal](http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf) part of speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame()\n",
    "pos_df['token'] = nltk.word_tokenize(short_snippet)\n",
    "pos_df['PTB'] = nltk.pos_tag(pos_df['token']) #, tagset='en-ptb')\n",
    "pos_df['PTB'] = pos_df['PTB'].map(lambda x: x[1])\n",
    "pos_df['Universal'] = nltk.pos_tag(pos_df['token'], tagset='universal')\n",
    "pos_df['Universal'] = pos_df['Universal'].map(lambda x: x[1])\n",
    "pos_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_2\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question 3.2:** What is the PTB tag and what is the Universal tag for the token `people`? What is different about these tags and what do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_3\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question 3.3:** What are the PTB tags for the 4th token (were), 5th token (trying) and 12th token (talk)? What do the differences mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_4\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 3.4:** Complete the missing code in the next cell to loop through each token in `snippet_doc` add the Spacy simple part of speech tag for each token to `pos_df`. Assign the new column in `pos_df` the name `Spacy_Simple_POS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pos = []\n",
    "snippet_doc = nlp(short_snippet)\n",
    "\n",
    "for tok in snippet_doc:\n",
    "...\n",
    "\n",
    "pos_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_5\n",
    "points: 2\n",
    "manual: false\n",
    "-->\n",
    "\n",
    "**Question 3.5:** Complete the missing code in the next cell to add the Spacy detailed part of speech tag for each token to `pos_df`. Assign the new column in `pos_df` the name `Spacy_Detailed_POS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pos = []\n",
    "snippet_doc = nlp(short_snippet)\n",
    "\n",
    "for tok in snippet_doc:\n",
    "...\n",
    "\n",
    "pos_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_6\n",
    "points: 1\n",
    "-->\n",
    "**Question 3.6:** What are the tokens where the Spacy Simple POS tag is different than the Universal tag in nltk? Assign them to the variable named `different_tags` which should be a set or numpy array of the unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_tags = ...\n",
    "different_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "It is possible that some tags might be different but convey more or less the same information. For example, periods are given the `Universal` tag of `.` in nltk but in `spacy` they have the `PUNCT`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_7\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question 3.7:** Looking at the tags for these terms, which of these tokens have very different tags between nltk and spacy and why might that be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## 4 Lemmatization in nltk\n",
    "\n",
    "The following line of code prints out morphological substitutions that are used for lemmatization in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.MORPHOLOGICAL_SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4_1\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "**Question 4.1:** Briefly describe what is printed out above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "In Monday's class we saw how the above rules change how we lemmatize a word based on its pos tag.\n",
    "Run the next line to see how we lemmatize `leaves` depending on its POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"leaves\", 'v'), lemmatizer.lemmatize(\"leaves\", 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4_2\n",
    "points: 2\n",
    "manual: true\n",
    "-->\n",
    "**Question 4.2:** In the next cell, come up with another word that has different lemmas based on its part of speech. Feel free to use the substitutions printed in 4.1 as a guide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## 5. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line will read in stopwords from three popular NLP libraries. \n",
    "`sklearn` is a library used for Machine Learning, we will cover it later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sw = set(nltk.corpus.stopwords.words('english'))\n",
    "spacy_sw = spacy.lang.en.stop_words.STOP_WORDS\n",
    "sklearn_sw = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_1\n",
    "points: 1\n",
    "manual: false\n",
    "-->\n",
    "\n",
    "**Question 5.1:** How many words are in each of the lists of stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_sklearn_sw = ...\n",
    "number_spacy_sw = ...\n",
    "number_nltk_sw = ...\n",
    "\n",
    "number_sklearn_sw, number_spacy_sw, number_nltk_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_2\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 5.2:** What words are unique to the stopword list in sklearn? In other words, what words are in sklearn's stop list that are not in the other two lists? Assign these words to the variable named `unique_sklearn_sw`.\n",
    "\n",
    "*Hint:* You might want to use the method from python `set`s to determine the values that are in one set but not in another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sklearn_sw = ...\n",
    "unique_sklearn_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_3\n",
    "points: 1\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "**Question 5.3:** Looking at these examples, what might be an example where we might not want to use this list of stop words? In otherwords, give an example of a type of research question or specific domain where removing some of these words would be a bad idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_4\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question: 5.4** What words are unique to the stopword list in nltk? In other words, what words are in nltk's stop list that are not in the other two lists? Assign these words to the variable named `unique_nltk_sw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nltk_sw = ...\n",
    "unique_nltk_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_5\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 5.5:** Is there a category in particular you notice about these words that are unique to nltk's stopswords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_6\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 5.6:** What words are unique to the stopword list in spacy? In other words, what words are in spacy's stop list that are not in the other two lists? Assign these words to the variable named `spacy_sklearn_sw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_spacy_sw = ...\n",
    "unique_spacy_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_7\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 5.7:** Comparing the `unique_spacy_sw` with `unique_nltk_sw`, what do you think might be causing these differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This paper](https://www.aclweb.org/anthology/W18-2502.pdf) discusses issues with blindly using stopwords from open source libraries. Specificially, \n",
    "\n",
    ">We have found\n",
    "that popular stop lists, which users often apply\n",
    "blindly, may suffer from surprising omissions and\n",
    "inclusions, or their incompatibility with particular\n",
    "tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson here is to be careful when applying existing lists of stopwords when cleaning your corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring the obituary\n",
    "\n",
    "This section demonstrates how we can explore the obituary. There is one question at the end of this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function that creates a dataframe called obit_pos_df where each row represents a token. The columns of the dataframe are:\n",
    "- `Word` (the original surface form of the token)\n",
    "- `lower` (the lowercased version of the token) \n",
    "- `POS` (the simplified universal pos tag of the token)\n",
    "- `lemma` (the lemma of the word)\n",
    "- `Stop Word` (boolean if the word is a stop word)\n",
    "- `Punctuation` (boolean if the word is punctuation)\n",
    "\n",
    "The cell will first pass the obit through the Spacy nlp pipeline and then apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_doc2pd(doc):\n",
    "    toks_df = pd.DataFrame()\n",
    "    toks_df[\"Word\"] = [word.text for word in doc]\n",
    "    toks_df[\"Lower\"] = [word.lower_ for word in doc] \n",
    "    toks_df[\"POS\"]  = [word.pos_ for word in doc] \n",
    "    toks_df[\"Lemma\"] =[word.lemma_ for word in doc] \n",
    "    toks_df[\"Stop Word\"] =[word.is_stop for word in doc]  \n",
    "    toks_df[\"Punctuation\"] =[word.is_punct for word in doc] \n",
    "    return toks_df\n",
    "\n",
    "obit = open(\"data/sparck-jones-obit.txt\").read()\n",
    "doc = nlp(obit)\n",
    "obit_df = spacy_doc2pd(doc)\n",
    "obit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell makes a barplot to show how many words have each Part of Speech Tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = obit_df['POS'].value_counts().plot(kind='bar', rot=45)\n",
    "ax.set_title(\"Count of POS tags in Spark Jones' obit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While nouns are the most frequent type of POS used, the most common type is not a noun. Rather it is a comman "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit_df['Lower'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at lemmas instead, we can see some differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit_df['Lemma'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove punctation. The next cell will print the most common lemmas that are not puncutation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit_df[~obit_df['Punctuation']]['Lemma'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many of these lemmas are function words rather than content words.\n",
    "The next line will determine the most common lemmas that are not a stop word or punctiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obit_df[(~obit_df['Stop Word']) & (~obit_df['Punctuation']) ]['Lemma'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6_1\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "**Question 6.1:** Does this bag of words representation capture who Karen Spark Jones was based on reading her obituary? Are there, if so, what aspects about her life are missing in this representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
