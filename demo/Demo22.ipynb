{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb410e1b",
   "metadata": {},
   "source": [
    "# Demo 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c314c1f",
   "metadata": {},
   "source": [
    "## Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce155c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "moview_reviews = nltk.corpus.movie_reviews\n",
    "review_files = [(file_id, file_id.startswith(\"pos\")) for file_id in moview_reviews.fileids()]\n",
    "df = pd.DataFrame(review_files)\n",
    "df = df.rename(columns={0: \"file_name\", 1: \"gold-label\"})\n",
    "\n",
    "\n",
    "def read_mov_review(f_name):\n",
    "    return moview_reviews.open(f_name).read()\n",
    "\n",
    "df['review_text'] = df['file_name'].apply(read_mov_review)\n",
    "\n",
    "df = df.sample(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc3ebd",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8109af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = [gram for gram in ngrams(df['review_text'].iloc[0], 2)]\n",
    "bigrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d32bc",
   "metadata": {},
   "source": [
    "**Question:** What happened?\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "    Lets look at the contextual help\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    It requires a list\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bd909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccfce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1551e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefec47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0baeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d88f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    return \" \".join([\" \".join(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(review)]).split()\n",
    "\n",
    "df['cleaned_text'] = df['review_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ee6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [gram for gram in ngrams(df['cleaned_text'].iloc[0], 2)]\n",
    "bigrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad19d4e",
   "metadata": {},
   "source": [
    "**Question:** How could we get tri-grams from the first sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749585bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203b494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91648715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69829e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [gram for gram in ngrams(df['cleaned_text'].iloc[0], 3)]\n",
    "trigrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd198e",
   "metadata": {},
   "source": [
    "**Question:** How could we get 25-grams from the first sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e43ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gram25 = [gram for gram in ngrams(df['cleaned_text'].iloc[0], 25)]\n",
    "gram25[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240c800",
   "metadata": {},
   "source": [
    "### Google n-gram viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e68de",
   "metadata": {},
   "source": [
    "https://books.google.com/ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a64f2d",
   "metadata": {},
   "source": [
    "(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e7135",
   "metadata": {},
   "source": [
    "### n-grams as features in DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(df['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23b179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f277532",
   "metadata": {},
   "source": [
    "**Assignment:** \n",
    "Looking at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for CountVectorizer, how can we extract n-grams as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a049b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399942bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c098c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c58ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "two_gram_vectorizer.fit_transform(df['review_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ffece",
   "metadata": {},
   "source": [
    "**Question:** How many more features do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f67743",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(two_gram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636faedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "two_gram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883490c2",
   "metadata": {},
   "source": [
    "Let's look at if we include unigram and bigrams as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a86918",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_bi_gram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "uni_bi_gram_vectorizer.fit_transform(df['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uni_bi_gram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb88be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uni_bi_gram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bd5a9",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "How can we determine the most probable n-grams?\n",
    "\n",
    "(ask class before going back to slides)\n",
    "\n",
    "\n",
    "(back to slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671b0dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Language Modeling\n",
    "\n",
    "Let's now look at the complete work of Shakspeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://norvig.com/ngrams/shakespeare.txt\n",
    "!mv shakespeare.txt data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0878c",
   "metadata": {},
   "source": [
    "This corpus was already tokenized:\n",
    "    \n",
    ">The complete works of Shakespeare, tokenized so that there is a space between words and punctuation. From John DeNero. https://norvig.com/ngrams/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line.strip().split() for line in open(\"data/shakespeare.txt\").readlines() if line.strip()]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c75b4c",
   "metadata": {},
   "source": [
    "This is code from the textbook. It computes the log probabilities of a sentence based on shakspeaere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ebe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "smoothing = 0.001\n",
    "counts = defaultdict(lambda: defaultdict(lambda: smoothing))\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = ['*', '*'] + sentence + ['STOP'] \n",
    "    for u, v, w in nltk.ngrams(tokens, 3):\n",
    "        counts[(u, v)][w] += 1\n",
    "\n",
    "def logP(u, v, w):\n",
    "    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)\n",
    "].values()))\n",
    "\n",
    "def sentence_logP(S):\n",
    "    tokens = ['*', '*'] + S + ['STOP']\n",
    "    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens,\n",
    "3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fe862",
   "metadata": {},
   "source": [
    "Now we can compute the log probability of a sentence being written by Shakpseare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ac024",
   "metadata": {},
   "source": [
    "*Note:* In the equation we were dividing, so when we use logs we end up using subtraction. So here, the larger the number is the more likely it is written by shakspeare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbab7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_logP(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cd98f",
   "metadata": {},
   "source": [
    "NYTimes headline from today: \n",
    "> President Biden and Prime Minister Boris Johnson will emphasize a vision of recovery from the pandemic that builds on the special relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b737208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nytimes_headline = \"President Biden and Prime Minister Boris Johnson will emphasize a vision of recovery from the pandemic that builds on the â€œspecial relationship.\"\n",
    "sentence_logP(nytimes_headline.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98673d",
   "metadata": {},
   "source": [
    "### Language Models to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3599c",
   "metadata": {},
   "source": [
    "We can now use the language model (these probabilities) to generate new Shakspeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc53377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(u, v):\n",
    "    keys, values = zip(*counts[(u, v)].items()) \n",
    "    values = np.array(values)\n",
    "    values /= values.sum()\n",
    "    return keys[np.argmax(np.random.multinomial(1, values))]\n",
    "\n",
    "def generate():\n",
    "    result = ['*', '*']\n",
    "    next_word = sample_next_word(result[-2], result[-1]) \n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-2], result[-1]) \n",
    "        result.append(next_word)\n",
    "    return ' '.join(result[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from(word):\n",
    "    result = ['*', word]\n",
    "    next_word = sample_next_word(result[-2], result[-1])\n",
    "    result.append(word)\n",
    "    result.append(next_word)\n",
    "    while next_word != 'STOP':\n",
    "        next_word = sample_next_word(result[-2], result[-1]) \n",
    "        result.append(next_word)\n",
    "    return ' '.join(result[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from(\"Oh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c1bb0",
   "metadata": {},
   "source": [
    "Let's look at all the possible words we could get following \"Oh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[(\"*\", \"Oh\")].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18422f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from(\"Hamlet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308efb8",
   "metadata": {},
   "source": [
    "Let's sample some more together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9340647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525d12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef8c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b3499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "119c03c1",
   "metadata": {},
   "source": [
    "(back to slides)\n",
    "\n",
    "## Finding Common Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad76215",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\" \".join(sent) for sent in corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stopwords_ = set(stopwords.words('english'))\n",
    "words = [word.lower() for document in documents for word in document.split()\n",
    "    if len(word) > 2\n",
    "    and word not in stopwords_]\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(words) \n",
    "bgm = BigramAssocMeasures()\n",
    "collocations = {bigram: pmi for bigram, pmi in finder.\n",
    "                    score_ngrams(bgm.mi_like)} \n",
    "\n",
    "collocations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
