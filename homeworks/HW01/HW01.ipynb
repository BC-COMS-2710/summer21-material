{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"HW01.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 - Presidential Inaugural Readability\n",
    "\n",
    "In this assignment, we will be analyzing the reading level of Presidential Inagural Speeches. \n",
    "By the end of this homework you will be able to answer questions such:\n",
    "- How has the reading level of Presidential Inagural Speeches changed over time?\n",
    "- Which President gave the longest Inagural Speech?\n",
    "- Which political party's Presidents' Inagural Speech had the highest and lowest reading levels?\n",
    "\n",
    "\n",
    "This assignment is inspired the following related work:\n",
    "- [The Atlantic](https://www.theatlantic.com/politics/archive/2014/10/have-presidential-speeches-gotten-less-sophisticated-over-time/381410/)\n",
    "- [Huffington Post](https://www.huffpost.com/entry/trump-speeches-reading-level_n_56e9899fe4b0b25c9184183f?mkv86w29=) (Work by Elliot Schumacher - my grad school labmate)\n",
    "- [The state of our union is … dumber (The Guardian)](https://www.theguardian.com/world/interactive/2013/feb/12/state-of-the-union-reading-level)\n",
    "\n",
    "There are python libraries, e.g. [textstat](https://github.com/shivam5992/textstat), that compute some of the metrics we will be using here. However, in this assignment we will implement different readability metrics by scratch. \n",
    "\n",
    "\n",
    "For all problems that you must write explanations and sentences for, you **must** provide your answer in the designated space. Moreover, throughout this homework and all future ones, please be sure to not re-assign variables throughout the notebook! For example, if you use `max_temperature` in your answer to one question, do not reassign it later on. Otherwise, you will fail tests that you thought you were passing previously!\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "This assignment is due Monday, May 10th at 11:45 A.M. Late work be accepted, up to one day per assignment, as per the [policies](http://coms2710.barnard.edu/policies.html) page.\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Refer to the policies page to learn more about how to learn cooperatively.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. Office hours are held Monday-Friday. The schedule appears on [Office Hours](http://coms2710.barnard.edu/office-hours.html) page.\n",
    "\n",
    "Let's begin by running the next cell that will import some python packages relevant for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore & Load Data\n",
    "\n",
    "When working with and analyzing data, it is always a good idea to try to get a sense of what the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.1\n",
    "manual: True\n",
    "points: 2\n",
    "-->\n",
    "\n",
    "**Question 1.1:** Write a bash command in the next cell that will print out the first ten lines of George Washington's Inaugural Speech in 1789. \n",
    "\n",
    "*Hint 1: You might want to look at the [explanation of common bash commands on the course webpage](http://coms2710.barnard.edu/bash-commands.html)*\n",
    "\n",
    "*Hint 2: Remember that to run a bash command in a JupyterNotebook, we start the command with \"!\". For example, `!cat data/speeches/2005-George_W._Bush.txt` will print out all of George W Bush's 2005 Inaugrual Speech.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "By looking at the first ten lines of George Washington's 1789 Inaugural Speech, you should notice that each line represents a new sentence in a speech. This information will be helpful for the next question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.2\n",
    "manual: false\n",
    "points: \n",
    "    - 0.15\n",
    "    - 0.10\n",
    "    - 0.10\n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 4\n",
    "-->\n",
    "\n",
    "**Question 1.2:** Fill in the missing code in the next cell to store the contents of each speech in a dataframe called `speeches_df`. The dataframe should contains the following columns: `Year`, `President`, `Speech`. `os.listdir(SPEECH_PATH)` results in a list of the names of the files in `SPEECH_PATH`.\n",
    "- For the `President` column, make sure to use a white space to seperate First and Last names for Presidents. Also, make sure to use *title* case (\"Adam Z Poliak\" is an example of title case while \"Adam z poliak\" is not).\n",
    "- Each item in the `Speech` column should be a list of strings. The *i-th* string in the list should correspond to the *i-th* sentence in the speech. Also, make sure to remove trailing white spaces.\n",
    "- The `Year` column should be a column of integers, not strings.\n",
    "\n",
    "*Hint: For title case, python strings have a useful function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_PATH=\"data/speeches/\"\n",
    "\n",
    "for file in os.listdir(SPEECH_PATH):\n",
    "    \"\"\"\n",
    "    Fill in the rest of this cell to iterate through the speeches in SPEECH_PATH. \n",
    "    For each speech, make sure to keep track\n",
    "    of the year, president, and contents of each speech.\n",
    "    \"\"\";\n",
    "    ...\n",
    "\n",
    "    \n",
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the dataframe\n",
    "Run the next line to look at the first 7 rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.3\n",
    "manual: True\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "Above, there most likely is not a clear order of the rows. \n",
    "\n",
    "**Question 1.3:** Why are the rows ordered in the way they are?\n",
    "    \n",
    "*Hint:* Remember the starter code in Question 1.2. This [documentation](https://docs.python.org/3/library/os.html#os.listdir) might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.4\n",
    "manual: false\n",
    "points: \n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    - 1\n",
    "-->\n",
    "\n",
    "**Question 1.4:** Sort the dataframe chronologically in ascending order so that the first row in the Dataframe corresponds to the first Presidential Inaugrual address. Make sure to reindex the dataframe.\n",
    "The first row should correspond to George Washington's Inaugrual address in 1789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5), speeches_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1.5\n",
    "manual: True\n",
    "points: 2\n",
    "-->\n",
    "**Question 1.5:** Briefly (maximum of 2 sentences) what do you think might make a speech or text more readable? List at least two aspects or characteristics might make a speech or text hard to read and understand? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## 2. Speech Statistics\n",
    "\n",
    "Before we implement different functions to determine the reading level of a speech, we will first compute the following statistics for each speech:\n",
    "\n",
    "- Number of words\n",
    "- Number of sentences\n",
    "- Number of syllabus per word\n",
    "- Number of pollysyllablic words\n",
    "- Number of characters\n",
    "\n",
    "These statistics will be used for different readability metrics in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Number of sentences\n",
    "\n",
    "Let's begin by determining the number of sentences in each speech.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.1\n",
    "manual: false\n",
    "points: \n",
    "    - 0.25\n",
    "    - 0.75\n",
    "    - 0.75\n",
    "-->\n",
    "\n",
    "**Question: 2.1** Compute the number of sentences in each speech and store the result in a new column called `Sentence_Count` in the Dataframe called `speeches_df` *(even though my solution was one line, it is okay if have one line of code for each step in your solution)*.\n",
    "\n",
    "*Hint:* Remember [`map`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html) from pandas and lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2.1\n",
    "manual: True\n",
    "points: \n",
    "    - 1\n",
    "-->\n",
    "**Question: 2.2.1** Remember from above that each line in the original file contained just one sentence. Briefly describe what bash commands could we use to find the number of sentences in any of the speeches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2.2\n",
    "manual: True\n",
    "points: \n",
    "    - 1\n",
    "-->\n",
    "**Question: 2.2.2** In the next cell, write and run that bash command to determine the number of lines that are in file that contains Geroge W. Bush's 2005 speech. You will need to change the next cell from a markdown cell to a code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2.3\n",
    "manual: True\n",
    "points: \n",
    "    - 1\n",
    "-->\n",
    "**Question: 2.2.3** In the next cell, write and run that bash command to determine the number of lines that are in the file that contains Thomas Jefferson's second inaugrual speech. You will need to change the next cell from a markdown cell to a code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.2.4\n",
    "manual: False\n",
    "points: \n",
    "    - 0\n",
    "    - 0.1\n",
    "    - 1\n",
    "    - 1\n",
    "-->\n",
    "**Question: 2.2.4** In the next cell, write code to extract from `speeches_df` the number of speeches in Bush's 2005 speech and Jefferson's second speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jefferson_num_sents = ...\n",
    "bush_num_sents = ...\n",
    "\n",
    "f\"Bush's 2005 speech had {bush_num_sents} sentences and Jefferson's second speech had {jefferson_num_sents} sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just did is an example of sanity checking. Ideally, the results should be the same. However, you should notice that there is a difference of 1 between the results querying the dataframe compared to using the command line function that counts the number of lines.\n",
    "\n",
    "For this specific example, right now that difference of 1 is ok. We will discuss why this is the case here but if you are interested, consider why the bash command resulted in one less line number compared to querying the dataframe for the number of sentences in a speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words\n",
    "\n",
    "The next statistic we will compute is the number of words in a speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.3\n",
    "manual: false\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.40\n",
    "    - 0.40\n",
    "-->\n",
    "**Question: 2.3** Implement the function `get_total_word_count` based on the docstring below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_word_count(speech):\n",
    "    \"\"\"\n",
    "    Given a speech (a list of strings), return the number of words in the speech.\n",
    "    Here, we will define words as a run of characters between whitespace.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "test = [\"one\", \n",
    "        \"one two\", \n",
    "        \"one two three\", \n",
    "        \"one two three four\", \n",
    "        \"one two three four five\",\n",
    "        \"one two three four five six.\"]\n",
    "get_total_word_count(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.4\n",
    "manual: false\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "-->\n",
    "**Question: 2.4** Use `get_total_word_count` to computer the total number of words in each speech and store the result in a new column called `Total_Word_Count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.5\n",
    "manual: false\n",
    "points: \n",
    "    - 0.10\n",
    "    - 0.9\n",
    "-->\n",
    "\n",
    "**Question 2.5:** Compute the average number of words per sentence in each speech and store the result in a new column called `Avg_Word_Count`. Round the results to the nearest hundredth (2 decimals) using `np.round`\n",
    "\n",
    "*Hint:* You should use the two columns created in the last few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.6\n",
    "manual: false\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.5\n",
    "    - 0.5\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 2.6:** Which President and in what year had the longest sentences on average and what was the average number of words per sentence in the speech? Make sure to store the corresponding results in `president_longest_sentences`, `year_longest_sentences`, `avg_longest_sentences_count`.\n",
    "\n",
    "*You do not need to complete the first two lines, they are there as a hint for the way I answered this question*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_avg_sent = ...\n",
    "longest_row = ...\n",
    "\n",
    "president_longest_sentences = ...\n",
    "year_longest_sentences = ...\n",
    "avg_longest_sentences_count = ...\n",
    "\n",
    "f\"{president_longest_sentences} averaged {avg_longest_sentences_count} words per sentence in {year_longest_sentences}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "####  Syllables\n",
    "So far the statistics we've computed are at the word level. However, words with many syllables are often associated with more complex reading levels. In the next few questions we will focus on getting statistics based on the number of syllables in a word.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.1\n",
    "manual: True\n",
    "points: 2  \n",
    "-->\n",
    "\n",
    "**Question 2.7.1:** In the next cell, briefly describe how would you go about counting the number of syllables in a word programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "When programming, we do not want to reinvent the wheel and a specific problem we are facing e.g. counting the number of syllables in a word, is often not unique to our specific use case. In these cases, it is useful to leverage exsiting python libraries/packages that implement the solution we are looking for. \n",
    "\n",
    "#### PIP\n",
    "pip (which stands for \"pip installs packages\") is a command-line system to install and manage python packages. Many python packages can be installed via pip. You can search for any package on the Python Package Index (PyPI) [webpage](https://pypi.org/). [W3school](https://www.w3schools.com/python/python_pip.asp) gives a good overview of pip. \n",
    "\n",
    "Counting the number of syllables in a word is not that uncommon, especially when analyzing the readability of text. Instead of implementing an algorithm to compute the number of syllabues in a word ourselves, we will use a publicly available python package called [`python-syllables`](https://github.com/prosegrinder/python-syllables). \n",
    "\n",
    "Run the next cell to install the python package using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the syllables package now, we need to import the package in python. Run the next code cell to import the `syllables` python package. The line of code will tell us the version number of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "syllables.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.2\n",
    "manual: True\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.75\n",
    "    - 0.75\n",
    "    \n",
    "-->\n",
    "\n",
    "Now we have access to the python library called `syllables`. \n",
    "\n",
    "**Question: 2.7.2:** Based on the package's [documentation](https://github.com/prosegrinder/python-syllables), use the package to determine how many syllables are in the words `hello`, `supercalifragilisticexpialidocious`? \n",
    "Assign the result of the expression to the corresponding variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_syl_count = ...\n",
    "super_syl_count = ...\n",
    "\n",
    "f\"There are {hello_syl_count} syllables in hello and {super_syl_count} syllables in supercalifragilisticexpialidocious\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.7.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.3\n",
    "manual: True\n",
    "points: 2\n",
    "    \n",
    "-->\n",
    "\n",
    "Often times we will use existing packages as black boxes. However, it is sometimes useful to understand what is going on under the hood of a function in a package. \n",
    "The package's `estimate()` function is implemented in these [13 lines of code](https://github.com/prosegrinder/python-syllables/blob/630c08786c72459e5a961503fefa5acb967f8d30/syllables/__init__.py#L176-L199)\n",
    "\n",
    "**Question 2.7.3:** Briefly (in at most 3 sentences) describe in English how the `estimate()` function works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.4\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question: 2.7.4** Implement the function `get_total_syllable_count` based on the docstring below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_syllable_count(speech):\n",
    "    \"\"\"\n",
    "    Given a speech (a list of strings), return the number of syllables in the speech.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "test = [\"welcome to the jungle\", \"we got fun\"]\n",
    "get_total_syllable_count([\"hello\"]), get_total_syllable_count(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.7.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.5\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "  \n",
    "  \n",
    "-->\n",
    "**Question: 2.7.5** Use `get_total_syllable_count` to computer the total number of syllables in each speech and store the result in a new column called `Total_Syllable_Count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.7.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "####  Polysyllabic Words\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.6\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question: 2.7.6** Implement the function `get_polysyllable_word_count` based on the docstring below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polysyllable_word_count(speech):\n",
    "    \"\"\"\n",
    "    Given a speech (a list of strings), return the number of words that contain 3 or more syllables in the speech.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "test = [\"The SMOG grade is a readability metric\",\n",
    "        \"It estimates the years of education needed to understand a piece of writing.\"]\n",
    "get_polysyllable_word_count([\"hello\"]), get_polysyllable_word_count(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.7.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.7\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "  \n",
    "  \n",
    "-->\n",
    "**Question: 2.7.7** Use `get_polysyllable_word_count` to computer the total number of polysyllabic words in each speech and store the result in a new column called `Total_Polysyllable_Count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.7.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.7.8\n",
    "manual: True\n",
    "points: 2  \n",
    "-->\n",
    "\n",
    "We often have multiple options of which python packages to use. For example, `python-syllables` is not the only publicly available python package that we can use to count the number of syllables in a word. [SyllaPy](https://github.com/mholtzscher/syllapy) is one as well. \n",
    " \n",
    "**Question 2.7.8:** Give two possible reasons why we might choose one python package over another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Characters\n",
    "\n",
    "The next statistic we will compute is the number of characters in a speech. Remember that characters here are not entities in a story. [\"Characters are anything you type in a keyboard\"](https://www.pythonforbeginners.com/basics/string-manipulation-in-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.8\n",
    "manual: false\n",
    "points: \n",
    "    - 0.5\n",
    "    - 0.5\n",
    "-->\n",
    "**Question: 2.8** Implement the function `get_total_char_count` based on the docstring below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_char_count(speech):\n",
    "    \"\"\"\n",
    "    Given a speech (a list of strings), return the number of character in the speech.\n",
    "    Here, we will define words as a run of characters between whitespace, including punctiation.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "test = [\"one\", \n",
    "        \"one two\"]\n",
    "get_total_char_count(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2.9\n",
    "manual: false\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.9\n",
    "-->\n",
    "**Question: 2.9** Use `get_total_char_count` to computer the total number of characters in each speech and store the result in a new column called `Total_Char_Count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Readability Metrics\n",
    "\n",
    "We are now ready to determine the reading level of different speeches. We are going to implement the following four readability tests for English texts:\n",
    "- Flesch Reading Ease\n",
    "- Flesch Kincaid Grade Level\n",
    "- Simple Measure of Gobbledygook (SMOG)\n",
    "- Automated Readability Index (ARI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1.1\n",
    "manual: False\n",
    "points: 0\n",
    "-->\n",
    "### Flesch reading ease (FRES)\n",
    "\n",
    "The first readability metric we will compute is the Flesh reading ease score (FRES). \n",
    "The equation for FRES is:\n",
    "$$206.835 - 1.015 \\left( \\frac{\\text{words}}{\\text{sentences}} \\right) - 84.6 \\left( \\frac{\\text{syllables}}{\\text{words}} \\right)$$\n",
    "\n",
    "Where *words* is the number words, *sentence* is the number of sentences, and *syllables* is the number of syllables. \n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease) for more information about this metric.\n",
    "\n",
    "\n",
    "**Question 3.1.1**\n",
    "Complete the function `fres()` to compute the Flresch Reading Ease score for an entire speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fres(row):\n",
    "    \"\"\"\n",
    "    Given a row (speech) in the dataframe, return the Flesch reading ease score of the speech in the row.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1.2\n",
    "manual: False\n",
    "points: \n",
    "    - 0.2\n",
    "    - 0.8\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.1.2:** As mentioned on Wikipedia, the Flesch Reading Ease Score for the sentence \"*The cat sat on the mat*\" should be 116. Fill in the following dataframe `cat_mat_df` to test that `fres` function works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mat_df = pd.DataFrame().assign(Speech = [[\"The cat sat on the mat.\"]])\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "cat_fres_score = ...\n",
    "\n",
    "f\"The fres score for The cat sat on the mat is {cat_fres_score}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1.3\n",
    "manual: False\n",
    "points: \n",
    "    - 0.2\n",
    "    - 0.8\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.1.3:** In the next cell, use the DataFrame [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) function to apply the `fres()` function to each speech and store the resulting pandas Series in the variable names `fres_scores`.\n",
    "Then, add the Series to the dataframe in a column called `Fres_Score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fres_scores = ...\n",
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1.4\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.1.4:** Based on the FRES metric, compute which President's speech and in which year was the easiest to read. Assign the value of the expressions to `easiest_fres_pres`, `easiest_fres_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easiest_fres_pres = ...\n",
    "easiest_fres_year = ...\n",
    "\n",
    "f\"{easiest_fres_pres} speech in {easiest_fres_year} was the easiest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.1.5\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.1.5:** Based on the FRES metric, compute which President's speech and in which year was the hardest to read.Assign the value of the expressions to `hardest_fres_pres`, `hardest_fres_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hardest_fres_pres = ...\n",
    "hardest_fres_year = ...\n",
    "\n",
    "f\"{hardest_fres_pres} speech in {hardest_fres_year} was the hardest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2.1\n",
    "manual: False\n",
    "points: 0\n",
    "-->\n",
    "\n",
    "### Flesch–Kincaid grade level\n",
    "\n",
    "The second metric we will compute is the Flesch–Kincaid grade level metric. \n",
    "The Flesch–Kincaid Grade Level Formula is:\n",
    "$$0.39 \\left ( \\frac{\\mbox{words}}{\\mbox{sentences}} \\right ) + 11.8 \\left ( \\frac{\\mbox{syllables}}{\\mbox{words}} \\right ) - 15.59$$ \n",
    "\n",
    "\n",
    "Where *words* is the number words, *sentence* is the number of sentences, and *syllables* is the number of syllables. \n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level) for more information about this metric.\n",
    "\n",
    "\n",
    "**Question 3.1.1**\n",
    "Complete the function `fk_grade_level()` to compute the Flesch-Kincaid grade level score for an entire speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_grade_level(row):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a row (speech) in the dataframe, return the Flesch-Kincaid grade level score of the speech in the row.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2.2\n",
    "manual: False\n",
    "points: \n",
    "    - 0.5\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.2.2:** As mentioned on Wikipedia, the Flesch Reading Ease Score for the sentence \"*The Australian platypus is seemingly a hybrid of a mammal and reptilian creature*\" is an 11.3. \n",
    "\n",
    "Fill in the following dataframe `aus_df` to test that `fres` function works correctly. If the FK grade level is about 12.17, that is close enough here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_df = pd.DataFrame().assign(Speech = [[\"The Australian platypus is seemingly a hybrid of a mammal and reptilian creature\"]])\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "aus_fk_grade = ...\n",
    "\n",
    "f\"The fres score for The cat sat on the mat is {aus_fk_grade}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2.3\n",
    "manual: True\n",
    "points: 1\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.2.3:** Why in your function does the FK grade level for \"*The Australian platypus is seemingly a hybrid of a mammal and reptilian creature*\" come out to about 12 rather than 11.3?\n",
    "\n",
    "*Hint:* Look at the values in `aus_df` and compare them with the description in Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2.4\n",
    "manual: False\n",
    "points: \n",
    "    - 0.25\n",
    "    - 0.25\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.2.4:** In the next cell, use the DataFrame [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) function to apply the `fk_grade_level` function to each speech and store the resulting pandas Series in the variable names `fk_grade_scores`.\n",
    "Then, add the Series to the dataframe in a column called `FK_Grade_Score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fk_grade_scores = ...\n",
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2.5\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.2.5:** Based on the FK Grade metric, compute which President's speech and in which year was the easiest to read. Assign the value of the expressions to `easiest_fk_pres`, `easiest_fk_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easiest_fk_pres = ...\n",
    "easiest_fk_year = ...\n",
    "\n",
    "f\"{easiest_fk_pres} speech in {easiest_fk_year} was the easiest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.2.6\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.2.6:** Based on the FRES metric, compute which President's speech and in which year was the hardest to read.Assign the value of the expressions to `hardest_fk_pres`, `hardest_fk_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hardest_fk_pres = ...\n",
    "hardest_fk_year = ...\n",
    "\n",
    "f\"{hardest_fk_pres} speech in {hardest_fk_year} was the hardest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3.1\n",
    "manual: False\n",
    "points: 0\n",
    "-->\n",
    "\n",
    "### Simple Measure of Gobbledygood (SMOG)\n",
    "\n",
    "The third metric we will compute is SMOG. \n",
    "As describes on [Wikipeida](https://en.wikipedia.org/wiki/SMOG), the formula for computing SMOG is:\n",
    "$$1.0430 \\sqrt{\\mbox{polysyllables}\\times{30 \\over \\mbox{sentences}} } + 3.1291$$  \n",
    "\n",
    "Where *polysyllables* is the number of polysyllables in a speech and *sentences* is the number of sentences in a speech.\n",
    "\n",
    "**Question 3.3.1:** Complete the function `SMOG()` to compute the SMOG metric for an entire speech.\n",
    "If the speech has less than 30 sentences, return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOG(row):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a row (speech) in the dataframe, return the SMOG score of the speech in the row.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3.2\n",
    "manual: False\n",
    "points: \n",
    "    - 0.25\n",
    "    - 0.25    \n",
    "-->\n",
    "\n",
    "**Question 3.3.2:** In the next cell, use the DataFrame [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) function to apply the `SMOG` function to each speech and store the resulting pandas Series in the variable names `smog_scores`.\n",
    "Then, add the Series to the dataframe in a column called `SMOG_Score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smog_scores = ...\n",
    "speeches_df = ...\n",
    "speeches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3.3\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.3.3:** Based on the SMOG score metric, compute which President's speech and in which year was the easiest to read. Assign the value of the expressions to `easiest_smog_pres`, `easiest_smog_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "easiest_smog_pres = ...\n",
    "easiest_smog_year = ...\n",
    "\n",
    "f\"{easiest_smog_pres} speech in {easiest_smog_year} was the easiest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.3.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.3.4\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.3.4:** Based on the SMOG metric, compute which President's speech and in which year was the hardest to read. Assign the value of the expressions to `hardest_smog_pres`, `hardest_smog_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hardest_smog_pres = ...\n",
    "hardest_smog_year = ...\n",
    "\n",
    "f\"{hardest_smog_pres} speech in {hardest_smog_year} was the hardest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.3.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Automated readability index (ARI)\n",
    "\n",
    "The fourth (and last) metric will use is Automated readability index (ARI). As described on [Wikipeida](https://en.wikipedia.org/wiki/Automated_readability_index), the formula for computing ARI is:\n",
    "$$4.71 \\left (\\frac{\\mbox{characters}}{\\mbox{words}} \\right) + 0.5 \\left (\\frac{\\mbox{words}}{\\mbox{sentences}} \\right)  - 21.43$$  \n",
    "\n",
    "where *characters*, *words*, *sentences* are the number of characters, words and sentences in a speech.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.4.1\n",
    "manual: False\n",
    "points: 0\n",
    "-->\n",
    "\n",
    "**Question 3.4.1:** Complete the function `ARI()` to compute the ARI metric for an entire speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARI(row):\n",
    "        \n",
    "    \"\"\"\n",
    "    Given a row (speech) in the dataframe, return the SMOG score of the speech in the row.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.4.2\n",
    "manual: False\n",
    "points: \n",
    "    - 0.25\n",
    "    - 0.25    \n",
    "-->\n",
    "\n",
    "**Question 3.4.2:** In the next cell, use the DataFrame [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) function to apply the `ARI` function to each speech and store the resulting pandas Series in the variable names `ari_scores`.\n",
    "Then, add the Series to the dataframe in a column called `ARI_Score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ari_scores = ...\n",
    "speeches_df = ...\n",
    "speeches_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.4.3\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.4.3:** Based on the ARI metric, compute which President's speech and in which year was the 2nd easiest to read. Assign the value of the expressions to `easiest_ari_pres`, `easiest_ari_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "easiest_ari_pres = ...\n",
    "easiest_ari_year = ...\n",
    "\n",
    "f\"{easiest_ari_pres}'s speech in {easiest_ari_year} was the second easiest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.4.4\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    - 0.1\n",
    "    - 0.9\n",
    "    - 0.9\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.4.4:** Based on the ARI metric, compute which President's speech and in which year was the second hardest to read.Assign the value of the expressions to `hardest_ari_pres`, `hardest_ari_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "hardest_ari_pres = ...\n",
    "hardest_ari_year = ...\n",
    "\n",
    "\n",
    "f\"{hardest_ari_pres}'s speech in {hardest_ari_year} was the second hardest Inaugural Address to read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.4.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5. Visualization\n",
    "\n",
    "There are other metrics for quantifying readability but for this assignment we will stick with these 4. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5.1\n",
    "manual: False\n",
    "points: \n",
    "    - 0.1\n",
    "    \n",
    "-->\n",
    "**Question 3.5.1** Create a dataframe called `metrics_df` that contain the columns `Year`, `President` and the metrics that were just computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = ...\n",
    "metrics_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.5.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5.2\n",
    "manual: False\n",
    "points: \n",
    "    - 0\n",
    "    - 1\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.5.2** We would like to determine whether there is a consistent change in readability of Inaugrual addresses over time. What visualization or type of figure would be best to determine whether there is a trend in readability of over time?\n",
    "\n",
    "- a. bar\n",
    "- b. scatter\n",
    "- c. line\n",
    "- d. histogram\n",
    "\n",
    "Assign the correct answer in the form `a`, `b`, `c`, or `d` to the variable `correct_visualization` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_visualization = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.5.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5.3\n",
    "manual: True\n",
    "points: 5\n",
    "    \n",
    "-->\n",
    "\n",
    "**Question 3.5.3** In the next cell, generate the type of graph answered in the last question. Make sure to make one plot for each of the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5.4\n",
    "manual: True\n",
    "points: 2   \n",
    "-->\n",
    "\n",
    "**Question 3.5.4** The Guardian article referenced on the top of this assignment analyzes Presidental State of the Union Addresses. How do your findings compare the findings from the Guardian article? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3.5.5\n",
    "manual: True\n",
    "points: 2   \n",
    "-->\n",
    "\n",
    "**Question 3.5.5** In the next cell, 1) describe another research question you could ask about the Inaugarual addresses based on these metrics, and 2) sketch out how you would go about answering this new question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 6. Additional Applications of Readability\n",
    "\n",
    "There are other readability metrics that have been developed. This [survey](https://community.ksde.org/LinkClick.aspx?fileticket=t8hDydbT-jo%3D&tabid=5575&mid=13625) contains an in-depth discuss of some of these other metrics if you are interested.\n",
    "\n",
    "Some of the readabiltiy metrics you implemented have been used to study language in other settings. For example, when  exploring whether advertising on\n",
    "chips targeted toward consumers of high socioeconomic\n",
    "status uses different language than that on chips designed to\n",
    "appeal to lower status consumers, Freedman and Jurafksy discovered that *expensive chips use more complex language than inexpensive chips* [link](https://web.stanford.edu/~jurafsky/freedmanjurafsky2011.pdf)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "manual: True\n",
    "points: 1   \n",
    "-->\n",
    "\n",
    "**Question 4:** With an eye towards your project, what other use cases can you think about using readability for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## 7. Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5.1\n",
    "manual: false\n",
    "points: 1   \n",
    "-->\n",
    "\n",
    "**Question:** Roughly how many hours did you spend on this assignment. Assign the total number of hours to the variable `time_spent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spent = ...\n",
    "time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5.2\n",
    "manual: True\n",
    "points: 1   \n",
    "-->\n",
    "\n",
    "**Optional:** Provide any comments or feedback below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
